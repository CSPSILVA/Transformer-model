{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addb40ec-a239-4ead-a36d-6184bdeb9f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "loss_function = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "def scaled_dot_product_attention(query, key, value):\n",
    "    matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "    depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "    logits = matmul_qk / tf.math.sqrt(depth)\n",
    "    attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "    output = tf.matmul(attention_weights, value)\n",
    "    return output, attention_weights\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        assert d_model % self.num_heads == 0\n",
    "        self.depth = d_model // self.num_heads\n",
    "        self.wq = Dense(d_model)\n",
    "        self.wk = Dense(d_model)\n",
    "        self.wv = Dense(d_model)\n",
    "        self.dense = Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, v, k, q):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "        scaled_attention, _ = scaled_dot_product_attention(q, k, v)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
    "        output = self.dense(concat_attention)\n",
    "        return output\n",
    "\n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "        Dense(dff, activation='relu'),\n",
    "        Dense(d_model)\n",
    "    ])\n",
    "\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "        # Project layer for input x\n",
    "        self.dense_proj = Dense(d_model)\n",
    "    def call(self, x, training):\n",
    "        attn_output = self.mha(x, x, x) # Multi-head attention\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        \n",
    "        # Project input x to have the same dimension as attn_output\n",
    "        x_proj = self.dense_proj(x)  # Use the projection layer here\n",
    "        out1 = self.layernorm1(x_proj + attn_output)\n",
    "        \n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "        return out2\n",
    "\n",
    "def build_transformer_model(sequence_length, feature_size, d_model=512, num_heads=8, dff=2048, rate=0.1):\n",
    "    inputs = Input(shape=(sequence_length, feature_size))\n",
    "    x = EncoderLayer(d_model, num_heads, dff, rate)(inputs)\n",
    "    x = Dense(1, activation='linear')(x)  # Output layer for forecasting\n",
    "    model = Model(inputs=inputs, outputs=x)\n",
    "    #model.compile(optimizer=Adam(learning_rate=1e-4), loss='huber_loss')\n",
    "    #model.compile(optimizer='adam', loss=loss_function, metrics=['accuracy'])\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4b5f64-bfb8-41a0-9b1a-4cb29cbb1945",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "train_features_path = 'D:/IIT/Y5/Final Year Project/FYP/Datasets/archive4_1/unisolar/Train and Test data sets/X_test.npy'  \n",
    "train_labels_path = 'D:/IIT/Y5/Final Year Project/FYP/Datasets/archive4_1/unisolar/Train and Test data sets/y_test.npy'  \n",
    "\n",
    "# Load your data\n",
    "X_train = np.load(train_features_path)\n",
    "y_train = np.load(train_labels_path)\n",
    "\n",
    "# Instantiate the model\n",
    "sequence_length = 24  # 24 time steps for one day of hourly data\n",
    "feature_size = 6  # Number of features in your dataset\n",
    "model = build_transformer_model(sequence_length, feature_size)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, batch_size=32, epochs=100, validation_split=0.2)\n",
    "\n",
    "# Save the model after training\n",
    "model.save('D:/IIT/Y5/Final Year Project/FYP/Datasets/archive4_1/unisolar/Train and Test data sets/solar_forecasting_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15d4e65-c7fc-449d-b2bd-db70d6102b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model after training\n",
    "model.save('D:/IIT/Y5/Final Year Project/FYP/Datasets/archive4_1/unisolar/Train and Test data sets/solar_forecasting_model.keras')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
