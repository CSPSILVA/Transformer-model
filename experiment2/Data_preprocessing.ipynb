{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed1282d-7e6b-4ac1-8525-61d44fbb75c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV files\n",
    "weather_data_path = 'D:/IIT/Y5/Final Year Project/FYP/Datasets/archive4/unisolar/Weather_Data_reordered_all.csv' \n",
    "solar_data_path = 'D:/IIT/Y5/Final Year Project/FYP/Datasets/archive4/unisolar/Solar_Energy_Generation.csv' \n",
    "\n",
    "weather_data = pd.read_csv(weather_data_path)\n",
    "solar_data = pd.read_csv(solar_data_path)\n",
    "\n",
    "# Select and rename the necessary columns from the weather data\n",
    "weather_filtered = weather_data[['Timestamp', 'CampusKey', 'AirTemperature', 'RelativeHumidity', 'WindSpeed', 'WindDirection']]\n",
    "\n",
    "# Select and filter the first dataset\n",
    "columns_weather_data = ['Timestamp', 'AirTemperature', 'RelativeHumidity', 'WindSpeed', 'WindDirection', 'CampusKey']\n",
    "weather_data_filtered = weather_data[columns_weather_data]\n",
    "weather_data_filtered = weather_data_filtered[weather_data_filtered['CampusKey'] == 1]\n",
    "\n",
    "# Select and rename the necessary columns from the solar generation data\n",
    "solar_filtered = solar_data[['Timestamp', 'CampusKey', 'SolarGeneration']]\n",
    "\n",
    "# Select and filter the second dataset\n",
    "columns_solar_data = ['Timestamp', 'SolarGeneration', 'CampusKey']\n",
    "solar_data_filtered = solar_data[columns_solar_data]\n",
    "solar_data_filtered = solar_data_filtered[solar_data_filtered['CampusKey'] == 1]\n",
    "\n",
    "# Replace missing values in 'column1' with 0\n",
    "solar_data_filtered_filled = solar_data_filtered.fillna({'SolarGeneration': 0})\n",
    "\n",
    "# Assuming weather_data_filtered the DataFrame\n",
    "columns_to_fill = ['AirTemperature', 'RelativeHumidity', 'WindSpeed', 'WindDirection']\n",
    "# Step 1 & 2: Calculate the mean for each Timestamp for the specified columns\n",
    "means_by_timestamp = weather_data_filtered.groupby('Timestamp')[columns_to_fill].transform('mean')\n",
    "# Step 3: Replace missing values with the calculated means\n",
    "weather_data_filtered[columns_to_fill] = weather_data_filtered[columns_to_fill].fillna(means_by_timestamp)\n",
    "\n",
    "# Ensure Timestamp columns are in datetime format to merge correctly\n",
    "weather_data_filtered['Timestamp'] = pd.to_datetime(weather_data_filtered['Timestamp'])\n",
    "solar_data_filtered_filled['Timestamp'] = pd.to_datetime(solar_data_filtered_filled['Timestamp'])\n",
    "\n",
    "# Merge the datasets on 'Timestamp' and 'CampusKey'\n",
    "merged_data = pd.merge(weather_data_filtered, solar_data_filtered_filled, on=['Timestamp', 'CampusKey'])\n",
    "\n",
    "# Drop any rows with missing values you can use:\n",
    "cleaned_data = merged_data.dropna()\n",
    "merged_data_unique = merged_data.drop_duplicates(subset=['Timestamp'], keep='first')\n",
    "\n",
    "# Save the cleaned dataset to a new CSV file\n",
    "cleaned_data.to_csv('D:/IIT/Y5/Final Year Project/FYP/Datasets/archive4/unisolar/Cleaned_Added_Merged_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40a02da-8b57-4a80-b673-508778884947",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV files\n",
    "cleaned_data_path = 'D:/IIT/Y5/Final Year Project/FYP/Datasets/archive4/unisolar/Cleaned_Added_Merged_data.csv' \n",
    "cleaned_data = pd.read_csv(cleaned_data_path)\n",
    "\n",
    "# Select and rename the necessary columns from the cleared data\n",
    "cleaned_filtered = cleaned_data[['Timestamp', 'AirTemperature', 'RelativeHumidity', 'WindSpeed', 'WindDirection', 'SolarGeneration']]\n",
    "\n",
    "columns_to_average = ['AirTemperature', 'RelativeHumidity', 'WindSpeed', 'WindDirection', 'SolarGeneration']\n",
    "\n",
    "# Step 1: Select the relevant columns (including 'Timestamp' for grouping)\n",
    "cleaned_data_selected = cleaned_filtered[['Timestamp'] + columns_to_average]\n",
    "\n",
    "# Step 2 & 3: Group by 'Timestamp' and calculate the mean for the specified columns\n",
    "average_per_timestamp = cleaned_data_selected.groupby('Timestamp').mean()\n",
    "\n",
    "# Reset the index to turn 'Timestamp' back into a column\n",
    "average_per_timestamp_reset = average_per_timestamp.reset_index()\n",
    "\n",
    "# Save the averaged dataset to a new CSV File, including 'Timestamp'\n",
    "average_per_timestamp_reset.to_csv('D:/IIT/Y5/Final Year Project/FYP/Datasets/archive4/unisolar/Cleaned_Added_Merged_Averaged_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37024c0-6c39-46ae-8327-6038a64a5e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Pearson correlation\n",
    "correlation_matrix = average_per_timestamp_reset[['SolarGeneration', 'AirTemperature', 'RelativeHumidity', 'WindSpeed', 'WindDirection']].corr()\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e43db0-311f-4bc7-befe-fbfaa77751f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV files\n",
    "solar_data_path = 'D:/IIT/Y5/Final Year Project/FYP/Datasets/archive4_1/unisolar/Solar_Energy_Generation.csv' \n",
    "solar_data = pd.read_csv(solar_data_path)\n",
    "\n",
    "# Replace missing values in 'column1' with 0\n",
    "solar_filled = solar_data.fillna({'SolarGeneration': 0})\n",
    "\n",
    "solar_filled.to_csv('D:/IIT/Y5/Final Year Project/FYP/Datasets/archive4_1/unisolar/Solar_Energy_Generation2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dbb8d5-33aa-4a3e-8d79-7ded82a2101f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV files\n",
    "weather_data_path = 'D:/IIT/Y5/Final Year Project/FYP/Datasets/archive4_1/unisolar/Weather_Data_reordered_all.csv' \n",
    "weather_data = pd.read_csv(weather_data_path)\n",
    "\n",
    "columns_to_fill = ['AirTemperature', 'RelativeHumidity', 'WindSpeed', 'WindDirection']\n",
    "# Step 1 & 2: Calculate the mean for each Timestamp for the specified columns\n",
    "means_by_timestamp = weather_data_filtered.groupby('Timestamp')[columns_to_fill].transform('mean')\n",
    "# Step 3: Replace missing values with the calculated means\n",
    "weather_data_filtered[columns_to_fill] = weather_data_filtered[columns_to_fill].fillna(means_by_timestamp)\n",
    "\n",
    "weather_data_filtered.to_csv('D:/IIT/Y5/Final Year Project/FYP/Datasets/archive4_1/unisolar/Weather_Data_reordered_all2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776ede0f-319d-45b3-9eef-625880d5d45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV files\n",
    "weather_data_path = 'D:/IIT/Y5/Final Year Project/FYP/Datasets/archive4_1/unisolar/Weather_Data_reordered_all2.csv' # Change to your actual file path\n",
    "solar_data_path = 'D:/IIT/Y5/Final Year Project/FYP/Datasets/archive4_1/unisolar/Solar_Energy_Generation2.csv' # Change to your actual file path\n",
    "\n",
    "weather_data = pd.read_csv(weather_data_path)\n",
    "solar_data = pd.read_csv(solar_data_path)\n",
    "\n",
    "# Select and filter the first dataset\n",
    "columns_weather_data = ['Timestamp', 'AirTemperature', 'RelativeHumidity', 'WindSpeed', 'WindDirection', 'CampusKey']\n",
    "weather_data_filtered = weather_data[columns_weather_data]\n",
    "weather_data_filtered = weather_data_filtered[weather_data_filtered['CampusKey'] == 1]\n",
    "\n",
    "# Select and filter the second dataset\n",
    "columns_solar_data = ['Timestamp', 'SolarGeneration', 'CampusKey']\n",
    "solar_data_filtered = solar_data[columns_solar_data]\n",
    "solar_data_filtered = solar_data_filtered[solar_data_filtered['CampusKey'] == 1]\n",
    "\n",
    "# Ensure Timestamp columns are in datetime format to merge correctly\n",
    "weather_data_filtered['Timestamp'] = pd.to_datetime(weather_data_filtered['Timestamp'])\n",
    "solar_data_filtered['Timestamp'] = pd.to_datetime(solar_data_filtered['Timestamp'])\n",
    "\n",
    "weather_data_filtered.to_csv('D:/IIT/Y5/Final Year Project/FYP/Datasets/archive4_1/unisolar/weather2.csv', index=False)\n",
    "solar_data_filtered.to_csv('D:/IIT/Y5/Final Year Project/FYP/Datasets/archive4_1/unisolar/solar2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747ba31b-842f-49ad-927a-3e91da902a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the weather data\n",
    "weather_data_path = 'D:/IIT/Y5/Final Year Project/FYP/Datasets/archive4_1/unisolar/weather2.csv' \n",
    "weather_data = pd.read_csv(weather_data_path)\n",
    "weather_data['Timestamp'] = pd.to_datetime(weather_data['Timestamp'])\n",
    "\n",
    "# Load the solar data\n",
    "solar_data_path = 'D:/IIT/Y5/Final Year Project/FYP/Datasets/archive4_1/unisolar/solar2.csv' \n",
    "solar_data = pd.read_csv(solar_data_path)\n",
    "solar_data['Timestamp'] = pd.to_datetime(solar_data['Timestamp'])\n",
    "\n",
    "# Get the start and end time of the solar data\n",
    "start_time = solar_data['Timestamp'].min()\n",
    "end_time = solar_data['Timestamp'].max()\n",
    "\n",
    "# Filter the weather data to match the solar data's time period\n",
    "aligned_weather_data = weather_data[(weather_data['Timestamp'] >= start_time) & (weather_data['Timestamp'] <= end_time)]\n",
    "\n",
    "# Merge the datasets on 'Timestamp'\n",
    "merged_data = pd.merge(solar_data, aligned_weather_data, on='Timestamp', how='left')\n",
    "\n",
    "# drop any rows with missing values you can use:\n",
    "cleaned_data = merged_data.dropna()\n",
    "merged_data_unique = merged_data.drop_duplicates(subset=['Timestamp'], keep='first')\n",
    "\n",
    "# Save the cleaned dataset to a new CSV file\n",
    "cleaned_data.to_csv('D:/IIT/Y5/Final Year Project/FYP/Datasets/archive4_1/unisolar/cleaned_new.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5914e0a9-05a7-4548-b6e6-120613bc6b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the weather data\n",
    "cleaned_data_path = 'D:/IIT/Y5/Final Year Project/FYP/Datasets/archive4_1/unisolar/cleaned_new.csv' \n",
    "cleaned_data = pd.read_csv(cleaned_data_path)\n",
    "\n",
    "# Assuming 'merged_data' is your DataFrame, remove the column in-place without reassigning\n",
    "cleaned_new = cleaned_data.drop(['CampusKey_x', 'CampusKey_y'], axis=1)\n",
    "\n",
    "# Save the cleaned dataset to a new CSV file\n",
    "cleaned_new.to_csv('D:/IIT/Y5/Final Year Project/FYP/Datasets/archive4_1/unisolar/cleaned_new2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91570fd-3892-47bd-b3b0-e398135fd519",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV files\n",
    "cleaned_data_path = 'D:/IIT/Y5/Final Year Project/FYP/Datasets/archive4_1/unisolar/cleaned_new2.csv' \n",
    "cleaned_data = pd.read_csv(cleaned_data_path)\n",
    "\n",
    "# First, ensure the Timestamp column is in datetime format\n",
    "cleaned_data['Timestamp'] = pd.to_datetime(cleaned_data['Timestamp'])\n",
    "\n",
    "# Extract time features\n",
    "cleaned_data['hour'] = cleaned_data['Timestamp'].dt.hour\n",
    "cleaned_data['day_of_week'] = cleaned_data['Timestamp'].dt.dayofweek\n",
    "cleaned_data['month'] = cleaned_data['Timestamp'].dt.month\n",
    "cleaned_data['is_weekend'] = cleaned_data['Timestamp'].dt.weekday >= 5  # True for Saturdays and Sundays\n",
    "\n",
    "# Add lag features for SolarGeneration\n",
    "# Add lags for the previous 1, 2, 3, and 4 time steps (15 mins each)\n",
    "for lag in range(1, 5):\n",
    "    cleaned_data[f'SolarGeneration_lag_{lag}'] = cleaned_data['SolarGeneration'].shift(lag)\n",
    "\n",
    "# Rolling window statistics for SolarGeneration (1-hour rolling mean)\n",
    "window_size = 4  # For a 1-hour window in 15-minute increments\n",
    "cleaned_data['SolarGeneration_rolling_mean_1h'] = cleaned_data['SolarGeneration'].rolling(window=window_size).mean()  # 4*15min = 1 hour\n",
    "cleaned_data['SolarGeneration_rolling_std_1h'] = cleaned_data['SolarGeneration'].rolling(window=window_size).std()\n",
    "\n",
    "# Drop the initial rows that now have NaN values due to lag and rolling features\n",
    "cleaned_data.dropna(inplace=True)\n",
    "\n",
    "output_file_path = 'D:/IIT/Y5/Final Year Project/FYP/Datasets/archive4_1/unisolar/Lag_roll_data.csv' \n",
    "cleaned_data.to_csv(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6f6dfe-3e27-40a5-acc4-5f6b8ecf0e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "import pandas as pd\n",
    "\n",
    "cleaned_data_path = 'D:/IIT/Y5/Final Year Project/FYP/Datasets/archive4_1/unisolar/Lag_roll_data.csv'\n",
    "cleaned_data = pd.read_csv(cleaned_data_path)\n",
    "\n",
    "# Select features for scaling\n",
    "features = ['SolarGeneration', 'AirTemperature', 'RelativeHumidity', 'WindSpeed', \n",
    "            'SolarGeneration_lag_1', 'SolarGeneration_rolling_mean_1h']\n",
    "\n",
    "# Initialize the RobustScaler\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Fit and transform the features\n",
    "cleaned_data[features] = scaler.fit_transform(cleaned_data[features])\n",
    "\n",
    "# Now your data is scaled using Robust Scaling\n",
    "output_file_path = 'D:/IIT/Y5/Final Year Project/FYP/Datasets/archive4_1/unisolar/New2/scaled_data.csv'  \n",
    "cleaned_data.to_csv(output_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7114883-f350-4db5-bab1-9d95fa763dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump\n",
    "\n",
    "# Save the fitted scaler to a file\n",
    "scaler_file_path = 'D:/IIT/Y5/Final Year Project/FYP/Datasets/archive4_1/unisolar/New2/robust_scaler.joblib'\n",
    "dump(scaler, scaler_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a16992-8705-4a23-a4a1-a95f3a742379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Sequence Length\n",
    "sequence_length = 24  # For example, using 24 time steps to represent one day of hourly data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e4fb95-8e44-48c7-80cd-1b8892d035b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "scaled_data_path = 'D:/IIT/Y5/Final Year Project/FYP/Datasets/archive4_1/unisolar/New2/scaled_data.csv'\n",
    "scaled_data = pd.read_csv(scaled_data_path)\n",
    "\n",
    "for col in ['SolarGeneration', 'AirTemperature', 'RelativeHumidity', 'WindSpeed', 'SolarGeneration_lag_1', 'SolarGeneration_rolling_mean_1h']:\n",
    "    scaled_data[col] = scaled_data[col].astype('float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3403e395-c9c0-4465-9288-3d29f7f4ac36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Sequences\n",
    "features = ['SolarGeneration', 'AirTemperature', 'RelativeHumidity', 'WindSpeed', \n",
    "            'SolarGeneration_lag_1', 'SolarGeneration_rolling_mean_1h']\n",
    "\n",
    "# Initialize lists to store input sequences and their corresponding outputs\n",
    "# Placeholder lists for sequences and targets\n",
    "input_sequences = []\n",
    "output_sequences = []\n",
    "\n",
    "# Define chunk size\n",
    "chunk_size = 10000  \n",
    "\n",
    "# Process in chunks\n",
    "for start in range(0, len(scaled_data) - sequence_length, chunk_size):\n",
    "    end = min(start + chunk_size + sequence_length, len(scaled_data))\n",
    "    chunk = scaled_data.iloc[start:end]\n",
    "    for i in range(len(chunk) - sequence_length):\n",
    "        sequence = chunk[features].iloc[i:i+sequence_length].values\n",
    "        target = chunk['SolarGeneration'].iloc[i+sequence_length]\n",
    "        input_sequences.append(sequence)\n",
    "        output_sequences.append(target)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X = np.array(input_sequences, dtype='float32')\n",
    "y = np.array(output_sequences, dtype='float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2c5262-6ec3-4581-936f-a18dff0c7e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the Data into Training and Testing Sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "np.save('D:/IIT/Y5/Final Year Project/FYP/Datasets/archive4_1/unisolar/Train and Test data sets/X_train.npy', X_train)\n",
    "np.save('D:/IIT/Y5/Final Year Project/FYP/Datasets/archive4_1/unisolar/Train and Test data sets/y_train.npy', y_train)\n",
    "np.save('D:/IIT/Y5/Final Year Project/FYP/Datasets/archive4_1/unisolar/Train and Test data sets/X_test.npy', X_test)\n",
    "np.save('D:/IIT/Y5/Final Year Project/FYP/Datasets/archive4_1/unisolar/Train and Test data sets/y_test.npy', y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
